{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import copy\n",
    "from random import shuffle\n",
    "#import tqdm.notebook as tqdm\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime \n",
    "import sys, os \n",
    "from glob import glob \n",
    "import imageio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Unet Architecture </h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png\" height=500, width =1000></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 568, 568])\n"
     ]
    }
   ],
   "source": [
    "def DoubleConv2D(in_channels, out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=0),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=0),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "a = torch.randn(1, 1, 572, 572)\n",
    "\n",
    "b = DoubleConv2D(1, 64)\n",
    "\n",
    "c = b(a) \n",
    "#Size must be (568, 568, 64)\n",
    "print(c.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "def crop(input_tensor, target_tensor):\n",
    "    target_size = target_tensor.size()[2]\n",
    "    input_size = input_tensor.size()[2] \n",
    "    required = input_size - target_size \n",
    "    delta = required//2 \n",
    "\n",
    "    return input_tensor[:, :, delta:input_size-delta, delta:input_size-delta]  \n",
    "\n",
    "a = torch.rand(1, 512, 64, 64)\n",
    "b = torch.rand(1, 512, 56, 56)\n",
    "\n",
    "c = crop(a, b) \n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 572, 572])\n",
      "torch.Size([1, 2, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.double_conv2d_1 = DoubleConv2D(1, 64)\n",
    "        self.double_conv2d_2 = DoubleConv2D(64, 128)\n",
    "        self.double_conv2d_3 = DoubleConv2D(128, 256)\n",
    "        self.double_conv2d_4 = DoubleConv2D(256, 512)\n",
    "        self.double_conv2d_5 = DoubleConv2D(512, 1024)\n",
    "\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(1024, 512, 2, stride=2, padding=0)\n",
    "\n",
    "        self.up_conv_1 = DoubleConv2D(1024, 512)\n",
    "\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(512, 256, 2, stride=2, padding=0)\n",
    "\n",
    "        self.up_conv_2 = DoubleConv2D(512, 256)\n",
    "\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(256, 128, 2, stride=2, padding=0)\n",
    "\n",
    "        self.up_conv_3 = DoubleConv2D(256, 128)\n",
    "\n",
    "        self.up_trans_4 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "\n",
    "        self.up_conv_4 = DoubleConv2D(128, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, 2, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #Contracting Path\n",
    "        x1 = self.double_conv2d_1(X) \n",
    "        x2 = self.max_pool(x1)\n",
    "        x3 = self.double_conv2d_2(x2) \n",
    "        x4 = self.max_pool(x3) \n",
    "        x5 = self.double_conv2d_3(x4) \n",
    "        x6 = self.max_pool(x5)\n",
    "        x7 = self.double_conv2d_4(x6) \n",
    "        x8 = self.max_pool(x7)\n",
    "        x9 = self.double_conv2d_5(x8) \n",
    "        x10 = self.up_trans_1(x9)\n",
    "\n",
    "        x4_1 = crop(x7, x10)\n",
    "\n",
    "        x11 = self.up_conv_1(torch.cat((x10, x4_1), 1))\n",
    "        x12 = self.up_trans_2(x11) \n",
    "        x3_1 = crop(x5, x12) \n",
    "        x13 = self.up_conv_2(torch.cat((x12, x3_1), 1))\n",
    "\n",
    "\n",
    "        x14 = self.up_trans_3(x13) \n",
    "        x2_1 = crop(x3, x14) \n",
    "        x15 = self.up_conv_3(torch.cat((x14, x2_1), 1))\n",
    "\n",
    "        x15 = self.up_trans_4(x15) \n",
    "        x1_1 = crop(x1, x15) \n",
    "        x16 = self.up_conv_4(torch.cat((x15, x1_1), 1))\n",
    "\n",
    "        output = self.output(x16)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "a = torch.rand(1, 1, 572, 572)\n",
    "b = Unet()\n",
    "c = b(a) \n",
    "#Size must be (568, 568, 64)\n",
    "print(a.size())\n",
    "print(c.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH GPU",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
